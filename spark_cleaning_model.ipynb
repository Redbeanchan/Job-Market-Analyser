{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759a190d",
   "metadata": {},
   "source": [
    "# Spark Cleaning + Modeling (Benchmark Notebook)\n",
    "\n",
    "This notebook mirrors the non-spark cleaning/modeling workflow using straightforward PySpark APIs.\n",
    "\n",
    "## Included benchmarking controls\n",
    "- Fixed split and random seed (`randomSplit(..., seed=42)`)\n",
    "- Same evaluation metrics (R2, MAE, RMSE)\n",
    "- End-to-end runtime timing\n",
    "- Lightweight CPU/memory snapshots before and after major stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cec590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.18\" 2026-01-20\n",
      "OpenJDK Runtime Environment Temurin-17.0.18+8 (build 17.0.18+8)\n",
      "OpenJDK 64-Bit Server VM Temurin-17.0.18+8 (build 17.0.18+8, mixed mode, sharing)\n",
      "\n",
      "Python executable: C:\\Users\\Alex\\Desktop\\BigData\\Job-Market-Analyser\\Notebooks\\.venv\\Scripts\\python.exe\n",
      "JAVA_HOME: C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.18.8-hotspot\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob, subprocess, time\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    psutil = None\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "def resource_snapshot(tag: str):\n",
    "    if psutil is None:\n",
    "        print(f\"[{tag}] psutil not installed (pip install psutil for CPU/memory snapshots)\")\n",
    "        return\n",
    "    cpu_pct = psutil.cpu_percent(interval=0.3)\n",
    "    mem_pct = psutil.virtual_memory().percent\n",
    "    print(f\"[{tag}] CPU%: {cpu_pct:.1f} | Memory%: {mem_pct:.1f}\")\n",
    "\n",
    "# 1) Remove stale PySpark gateway env (can trigger insecure gateway error)\n",
    "for k in [\"PYSPARK_GATEWAY_PORT\", \"PYSPARK_GATEWAY_SECRET\"]:\n",
    "    os.environ.pop(k, None)\n",
    "\n",
    "# 2) Set JAVA_HOME dynamically (Temurin JDK)\n",
    "jdk_candidates = sorted(glob.glob(r\"C:\\Program Files\\Eclipse Adoptium\\jdk-*\"))\n",
    "if not jdk_candidates:\n",
    "    raise RuntimeError(\"No JDK found under C:\\\\Program Files\\\\Eclipse Adoptium\\\\jdk-*\")\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = jdk_candidates[-1]\n",
    "os.environ[\"PATH\"] = os.path.join(os.environ[\"JAVA_HOME\"], \"bin\") + \";\" + os.environ[\"PATH\"]\n",
    "\n",
    "# 3) Ensure Spark uses this notebook's Python\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# 4) Verify Java\n",
    "print(subprocess.check_output([\"java\", \"-version\"], stderr=subprocess.STDOUT).decode())\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"JAVA_HOME:\", os.environ[\"JAVA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12207aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] CPU%: 17.5 | Memory%: 32.5\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "employee_path = r\"C:\\Users\\Alex\\Downloads\\Employee_dataset.csv\"\n",
    "salary_path = r\"C:\\Users\\Alex\\Downloads\\Employee_salaries.csv\"\n",
    "\n",
    "assert os.path.exists(employee_path), f\"Missing file: {employee_path}\"\n",
    "assert os.path.exists(salary_path), f\"Missing file: {salary_path}\"\n",
    "\n",
    "run_start = time.perf_counter()\n",
    "resource_snapshot(\"start\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark_cleaning_model_benchmark\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126ddda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 1000000 1000000\n"
     ]
    }
   ],
   "source": [
    "# ---- Load + Standardize ----\n",
    "emp = (\n",
    "    spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(employee_path)\n",
    "    .withColumnRenamed(\"jobId\", \"job_id\")\n",
    "    .withColumnRenamed(\"companyId\", \"company_id\")\n",
    "    .withColumnRenamed(\"jobRole\", \"job_role\")\n",
    "    .withColumnRenamed(\"Industry\", \"industry\")\n",
    "    .withColumnRenamed(\"yearsExperience\", \"years_experience\")\n",
    "    .withColumnRenamed(\"distanceFromCBD\", \"distance_from_cbd\")\n",
    ")\n",
    "\n",
    "sal = (\n",
    "    spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(salary_path)\n",
    "    .withColumnRenamed(\"jobId\", \"job_id\")\n",
    "    .withColumnRenamed(\"salaryInThousands\", \"salary_in_thousands\")\n",
    ")\n",
    "\n",
    "print(\"Loaded rows:\", emp.count(), sal.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0c4f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned + merged rows: 999465\n",
      "[post_clean] CPU%: 18.0 | Memory%: 34.0\n"
     ]
    }
   ],
   "source": [
    "# ---- Cleaning (Spark equivalent of non-spark logic) ----\n",
    "\n",
    "# ID parsing from JOBxxxx / COMPxxxx\n",
    "emp = emp.withColumn(\"job_id\", F.regexp_extract(F.col(\"job_id\").cast(\"string\"), r\"(\\d+)\", 1).cast(T.LongType()))\n",
    "emp = emp.withColumn(\"company_id\", F.regexp_extract(F.col(\"company_id\").cast(\"string\"), r\"(\\d+)\", 1).cast(T.LongType()))\n",
    "\n",
    "sal = sal.withColumn(\"job_id\", F.regexp_extract(F.col(\"job_id\").cast(\"string\"), r\"(\\d+)\", 1).cast(T.LongType()))\n",
    "sal = sal.withColumn(\"salary_in_thousands\", F.col(\"salary_in_thousands\").cast(T.DoubleType()))\n",
    "\n",
    "# Normalize text fields\n",
    "emp = emp.withColumn(\"job_role\", F.upper(F.trim(F.col(\"job_role\"))))\n",
    "emp = emp.withColumn(\"industry\", F.upper(F.trim(F.col(\"industry\"))))\n",
    "\n",
    "# Fill education/major missing with NONE\n",
    "emp = emp.fillna({\"education\": \"NONE\", \"major\": \"NONE\"})\n",
    "\n",
    "# Cast numeric columns\n",
    "emp = emp.withColumn(\"years_experience\", F.col(\"years_experience\").cast(T.DoubleType()))\n",
    "emp = emp.withColumn(\"distance_from_cbd\", F.col(\"distance_from_cbd\").cast(T.DoubleType()))\n",
    "\n",
    "# Drop critical missing rows\n",
    "emp = emp.dropna(subset=[\"job_id\", \"company_id\", \"job_role\", \"industry\", \"years_experience\", \"distance_from_cbd\"])\n",
    "sal = sal.dropna(subset=[\"job_id\", \"salary_in_thousands\"])\n",
    "\n",
    "# Remove known invalid role from EDA\n",
    "emp = emp.filter(F.col(\"job_role\") != \"SCAMMER\")\n",
    "\n",
    "# Range filters\n",
    "emp = emp.filter((F.col(\"years_experience\") >= 0) & (F.col(\"years_experience\") <= 50))\n",
    "emp = emp.filter((F.col(\"distance_from_cbd\") >= 0) & (F.col(\"distance_from_cbd\") <= 100))\n",
    "sal = sal.filter((F.col(\"salary_in_thousands\") > 0) & (F.col(\"salary_in_thousands\") <= 500))\n",
    "\n",
    "# Merge + de-dup\n",
    "sdf = emp.join(sal, on=\"job_id\", how=\"inner\").dropDuplicates().dropDuplicates([\"job_id\"])\n",
    "\n",
    "print(\"Cleaned + merged rows:\", sdf.count())\n",
    "resource_snapshot(\"post_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0df6186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model rows: 999464\n"
     ]
    }
   ],
   "source": [
    "# ---- Feature Engineering (Spark) ----\n",
    "\n",
    "def map_from_dict(mapping: dict):\n",
    "    return F.create_map([F.lit(x) for kv in mapping.items() for x in kv])\n",
    "\n",
    "edu_map = {\"NONE\": 0, \"HIGH_SCHOOL\": 1, \"BACHELORS\": 2, \"MASTERS\": 3, \"DOCTORAL\": 4}\n",
    "role_map = {\"JANITOR\": 0, \"JUNIOR\": 1, \"SENIOR\": 2, \"MANAGER\": 3, \"VICE_PRESIDENT\": 4, \"CTO\": 5, \"CFO\": 5, \"CEO\": 6}\n",
    "industry_map = {\"EDUCATION\": 1, \"SERVICE\": 1, \"AUTO\": 2, \"HEALTH\": 3, \"WEB\": 4, \"FINANCE\": 5, \"OIL\": 5}\n",
    "major_map = {\"NONE\": 0, \"LITERATURE\": 1, \"BIOLOGY\": 2, \"CHEMISTRY\": 3, \"PHYSICS\": 4, \"COMPSCI\": 5, \"MATH\": 6, \"BUSINESS\": 7, \"ENGINEERING\": 8}\n",
    "\n",
    "sdf = sdf.withColumn(\"education_level\", map_from_dict(edu_map)[F.col(\"education\")].cast(T.DoubleType()))\n",
    "sdf = sdf.withColumn(\"job_role_rank\", map_from_dict(role_map)[F.col(\"job_role\")].cast(T.DoubleType()))\n",
    "sdf = sdf.withColumn(\"industry_score\", map_from_dict(industry_map)[F.col(\"industry\")].cast(T.DoubleType()))\n",
    "sdf = sdf.withColumn(\"major_score\", map_from_dict(major_map)[F.col(\"major\")].cast(T.DoubleType()))\n",
    "\n",
    "sdf = sdf.withColumn(\n",
    "    \"handcrafted_score\",\n",
    "    (F.col(\"education_level\") + F.col(\"job_role_rank\") + F.col(\"industry_score\") + F.col(\"major_score\")).cast(T.DoubleType())\n",
    ")\n",
    "sdf = sdf.withColumn(\"exp_sq\", (F.col(\"years_experience\") * F.col(\"years_experience\")).cast(T.DoubleType()))\n",
    "\n",
    "feature_cols = [\n",
    "    \"years_experience\",\n",
    "    \"distance_from_cbd\",\n",
    "    \"education_level\",\n",
    "    \"job_role_rank\",\n",
    "    \"industry_score\",\n",
    "    \"major_score\",\n",
    "    \"handcrafted_score\",\n",
    "    \"exp_sq\",\n",
    "]\n",
    "\n",
    "target_col = \"salary_in_thousands\"\n",
    "\n",
    "sdf = sdf.dropna(subset=feature_cols + [target_col])\n",
    "print(\"Model rows:\", sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726b56ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workflow</th>\n",
       "      <th>model</th>\n",
       "      <th>runtime_seconds</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spark</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>7.060047</td>\n",
       "      <td>0.741258</td>\n",
       "      <td>15.913671</td>\n",
       "      <td>19.702405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spark</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>8.559929</td>\n",
       "      <td>0.728262</td>\n",
       "      <td>16.210920</td>\n",
       "      <td>20.191133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  workflow             model  runtime_seconds        R2        MAE       RMSE\n",
       "0    spark  LinearRegression         7.060047  0.741258  15.913671  19.702405\n",
       "1    spark      RandomForest         8.559929  0.728262  16.210920  20.191133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Split + Train + Evaluate (same metrics) ----\n",
    "train_sdf, test_sdf = sdf.randomSplit([1 - TEST_SIZE, TEST_SIZE], seed=RANDOM_STATE)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "train_vec = assembler.transform(train_sdf).select(\"features\", F.col(target_col).alias(\"label\"))\n",
    "test_vec = assembler.transform(test_sdf).select(\"features\", F.col(target_col).alias(\"label\"))\n",
    "\n",
    "r2_eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "mae_eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "rmse_eval = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Linear Regression\n",
    "t0 = time.perf_counter()\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_vec)\n",
    "lr_pred = lr_model.transform(test_vec)\n",
    "lr_time = time.perf_counter() - t0\n",
    "rows.append({\n",
    "    \"workflow\": \"spark\",\n",
    "    \"model\": \"LinearRegression\",\n",
    "    \"runtime_seconds\": lr_time,\n",
    "    \"R2\": float(r2_eval.evaluate(lr_pred)),\n",
    "    \"MAE\": float(mae_eval.evaluate(lr_pred)),\n",
    "    \"RMSE\": float(rmse_eval.evaluate(lr_pred)),\n",
    "})\n",
    "\n",
    "# Random Forest (kept moderate for laptop stability)\n",
    "t0 = time.perf_counter()\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", numTrees=30, maxDepth=8, seed=RANDOM_STATE)\n",
    "rf_model = rf.fit(train_vec)\n",
    "rf_pred = rf_model.transform(test_vec)\n",
    "rf_time = time.perf_counter() - t0\n",
    "rows.append({\n",
    "    \"workflow\": \"spark\",\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"runtime_seconds\": rf_time,\n",
    "    \"R2\": float(r2_eval.evaluate(rf_pred)),\n",
    "    \"MAE\": float(mae_eval.evaluate(rf_pred)),\n",
    "    \"RMSE\": float(rmse_eval.evaluate(rf_pred)),\n",
    "})\n",
    "\n",
    "spark_results = pd.DataFrame(rows).sort_values(\"RMSE\").reset_index(drop=True)\n",
    "spark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1496df09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_salary_k</th>\n",
       "      <th>predicted_salary_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>102.617274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174.0</td>\n",
       "      <td>159.753291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130.0</td>\n",
       "      <td>143.509845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110.0</td>\n",
       "      <td>121.903034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>107.627587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76.0</td>\n",
       "      <td>77.908098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100.0</td>\n",
       "      <td>119.378546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>131.0</td>\n",
       "      <td>158.421987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>102.0</td>\n",
       "      <td>110.457632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>173.0</td>\n",
       "      <td>151.570753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>140.0</td>\n",
       "      <td>135.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>91.0</td>\n",
       "      <td>104.105676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>95.0</td>\n",
       "      <td>88.254954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>87.0</td>\n",
       "      <td>101.688575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>104.0</td>\n",
       "      <td>114.189385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual_salary_k  predicted_salary_k\n",
       "0             100.0          102.617274\n",
       "1             174.0          159.753291\n",
       "2             130.0          143.509845\n",
       "3             110.0          121.903034\n",
       "4             100.0          107.627587\n",
       "5              76.0           77.908098\n",
       "6             100.0          119.378546\n",
       "7             131.0          158.421987\n",
       "8             102.0          110.457632\n",
       "9             173.0          151.570753\n",
       "10            140.0          135.000672\n",
       "11             91.0          104.105676\n",
       "12             95.0           88.254954\n",
       "13             87.0          101.688575\n",
       "14            104.0          114.189385"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small prediction sample from best spark model\n",
    "best_name = spark_results.iloc[0][\"model\"]\n",
    "best_pred_df = rf_pred if best_name == \"RandomForest\" else lr_pred\n",
    "\n",
    "sample = best_pred_df.select(F.col(\"label\").alias(\"actual_salary_k\"), F.col(\"prediction\").alias(\"predicted_salary_k\")).limit(15)\n",
    "sample.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3b80b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[end] CPU%: 13.0 | Memory%: 35.0\n",
      "Total notebook runtime: 42.35 seconds\n"
     ]
    }
   ],
   "source": [
    "run_total = time.perf_counter() - run_start\n",
    "resource_snapshot(\"end\")\n",
    "print(f\"Total notebook runtime: {run_total:.2f} seconds\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
